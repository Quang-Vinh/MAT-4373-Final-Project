---
title: "Part 1 Option B - Handwritten Digit Recognition with Neural Networks"
author: "UnderGrad Team"
header-includes: \usepackage{graphicx}
# date: "12/04/2021"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}

library(ggthemes)
library(keras)
library(R.matlab)
library(tidyverse)
library(plot.matrix)
library(dplyr)
library(RColorBrewer)
library(tensorflow)


set.seed(42)
```

# Part 1

The code for generating the images in part 1


```{r}
rotate <- function(x) { t(apply(x, 2, rev))}
M = readMat("mnist_all.mat")
par(pty="s")
par(mfrow=c(10, 10))
par(mar = c(0, 0, 0, 0))

a <- c(5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949) 			#the number of training samples of each digit
	
b = sample.int(a[1], 10, replace=FALSE)         					#random sample of zeros
for(i in b){										#loop that print ten random zeros
	num <- matrix(M$train0[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}

b = sample.int(a[2], 10, replace=FALSE)							#prints ten random ones
for(i in b){
	num <- matrix(M$train1[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}

b = sample.int(a[3], 10, replace=FALSE)							#prints ten random twos ... etc.
for(i in b){
	num <- matrix(M$train2[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)	
}

b = sample.int(a[4], 10, replace=FALSE)
for(i in b){
	num <- matrix(M$train3[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}

b = sample.int(a[5], 10, replace=FALSE)	
for(i in b){
	num <- matrix(M$train4[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}

b = sample.int(a[6], 10, replace=FALSE)
for(i in b){
	num <- matrix(M$train5[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}

b = sample.int(a[7], 10, replace=FALSE)	
for(i in b){
	num <- matrix(M$train6[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}

b = sample.int(a[8], 10, replace=FALSE)	
for(i in b){
	num <- matrix(M$train7[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}

b = sample.int(a[9], 10, replace=FALSE)
for(i in b){
	num <- matrix(M$train8[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}

b = sample.int(a[10], 10, replace=FALSE)	
for(i in b){
	num <- matrix(M$train9[i,1:784],nrow = 28,ncol = 28)
	image(rotate(t(num)), col = gray.colors(256), axes = FALSE)
}


```


# Part 2

A function computing the output of the neural net given an $784$ x $1$ vector representing a digit

```{r}
softmax <- function(y){
	return(exp(y)/sum(exp(y)))
}

forward <- function(X, W, b){ 
  L1 <- W%*%X + b
  output <- softmax(L1)
  return(output)
}
```


# ```{r message=FALSE, warning=FALSE}
# library(R.matlab)
# library(plot.matrix)
# M = readMat("mnist_all.mat")
# ```

```{r warning=FALSE}
X_train = matrix(NA,784,0)
Y_train = matrix(NA,10,0)

for(i in 0:9){
  X_tmp = t(M[sprintf("train%d",i)][[1]])
  X_train = cbind(X_train,X_tmp)
  Y_tmp = matrix(0,10,dim(X_tmp)[2])
  Y_tmp[i,] = 1
  Y_train = cbind(Y_train,Y_tmp)
}

X_train = X_train/255
```

# Part 3  
The code is as follows. Function `deriv_multilayer` can calculate gradient of weight matrix $X$ and bias term $b$ using backpropagation method:  

```{r}
cost <- function(y,label){
  #Return the cost (neg log likelihood ) of predicted label y and true label .
  #y and label are both NxM matrix where N is the number of outputs for a single
  #case , and M is the number of cases 
  -sum(log(y)*label)
}

softmax <- function(y){
  #Return the output of the softmax function for the matrix of output y. y
  #is an NxM matrix where N is the number of outputs for a single case , and M
  #is the number of cases 
  exp(y)/outer(rep(1,dim(y)[1]),apply(exp(y),2,sum))
}

linear_layer <- function(y,W,b){
  t(W)%*%y+outer(b,rep(1,dim(y)[2]))
}

forward <- function(x,W,b){
  o = linear_layer(x,W,b)
  prob = softmax(o)
  return(prob)
}

deriv_multilayer <- function(x,W,b,y){
  prob = forward(x,W,b)
  dW = x%*%t(prob-y)
  db = apply(prob-y,1,sum)
  return(list(dW=dW,db=db))
}
```

And we calculate the gradient using following code:  
```{r}
# initialize W and b
W = matrix(rnorm(7840,sd = 0.1),784,10)
b = rnorm(10,sd=0.5)

idx_sample = sample(1:60000,200,replace=F)
X_train_sample = X_train[,idx_sample]
Y_train_sample = Y_train[,idx_sample]
info = deriv_multilayer(X_train,W,b,Y_train)
dW = info$dW
db = info$db
```

# Part 4  
We also develop the numerical method for calculation of gradient of $W$ and $b$, the code is as follows:  

```{r}
numeric_derive <- function(x,W,b,y,h=1e-5,rows=NULL,cols=NULL){
  s1 = dim(W)[1]
  s2 = dim(W)[2]
  prob0 = forward (x,W,b)
  cost0 = cost(prob0,y)
  if(is.null(rows)){
    rows = 1:s1
  }
  if(is.null(cols)){
    cols = 1:s2
  }
  
  dW = matrix(NA,s1,s2)
  db = rep(NA,s2)
  for(i in rows){
    for(j in cols){
      W_tmp = W
      W_tmp[i,j] = W_tmp[i,j]+h
      prob1 = forward(x,W_tmp,b)
      cost1 = cost(prob1,y)
      dW[i,j] = (cost1-cost0)/h
    }
  }
  
  for(j in 1:s2){
    b_tmp = b
    b_tmp[j] = b[j] + h
    prob1 = forward(x,W,b_tmp)
    cost1 = cost(prob1,y)
    db[j] = (cost1-cost0)/h
  }
  return(list(dW=dW,db=db))
}
```
For comparison and verification. We design a simple example such that, we initialize $W$ and $b$ such that $W_{ij}\sim N(0,0.01)$, $i=1,\dots,784$, $j=1,\dots,10$ and $b_j\sim N(0,0.05)$, $j=1,\dots,10$. And we use a sample of 500 data in train set to compute the gradient of $W$ and $b$ (using both backpropagation and numeric method) for this step.
The corressponding code are as follows:

```{r}
info = numeric_derive(X_train,W,b,Y_train,rows=111:120)
dW1 = info$dW
db1 = info$db
```

```{r}
plot(dW[111:115,],main="Gradient of W using backpropagation")
```
```{r}
plot(dW1[111:115,],main="Gradient of W using numeric method")
```

The above figures shows the colormap of a fraction of gradient $\frac{\partial L}{\partial W}$,($i=111,\dots,115$,$j=1,\dots,10$) computed by both backpropagation method(part3) and numerical method. We find that, the estimated values are almost the same for both method, which verify that we compute the gradient correctly. 


# Part 5

# Part 6

<!-- Add explanation -->

```{r}

# prepping format
par(mfrow=c(4,3))

# looping for each digit

for(i in 1:10){
  j=i-1
  
  # extract 28x28 matrix with weights for each digit/pixel
  num_j <- matrix(W[,i],nrow = 28,ncol = 28) %>%
    
    # clean up
    as_tibble() %>%
    rowid_to_column(var="x_position") %>%
    gather(key="y_position", value="weight", -1) %>%
    
    mutate(y_position=as.numeric(gsub("V","",y_position))) %>%
  
    # graph the heatmap
    ggplot(aes(x_position, y_position, fill= weight)) + 
      geom_tile() +
      theme_minimal() +
      scale_fill_distiller(palette = "RdBu") +
      ggtitle(paste("Heatmap of digit: ", j)) +
      theme(legend.position="none",
            axis.title=element_blank(),
            axis.text=element_blank(),
            axis.ticks=element_blank(),
            plot.title = element_text(size=14, face="bold.italic", hjust = 0.5))
  
  # print the graph
  print(num_j)

}
```


# Part 7

See https://tensorflow.rstudio.com/guide/keras/ for documentation. Here we define the neural network.

```{r}

# Load data
mnist <- readMat('mnist_all.mat')
data_train <- data.frame()
data_test <- data.frame()

for (i in 0:9) {
  train_digit <- mnist[paste0('train', i)][[1]] %>% data.frame
  train_digit['Y'] <- i
  data_train <- rbind(train_digit, data_train)
  
  test_digit <- mnist[paste0('test', i)][[1]] %>% data.frame
  test_digit['Y'] <- i
  data_test <- rbind(test_digit, data_test)
}

# Shuffle training dataset
data_train <- data_train[sample(nrow(data_train)), ]
data_test <- data_test[sample(nrow(data_test)), ]

# Split into X and Y
X_train <- data_train %>% select(-Y) %>% as.matrix()
Y_train <- data_train$Y
X_test <- data_test %>% select(-Y) %>% as.matrix()
Y_test <- data_test$Y

# Scale by 255
X_train <- X_train / 255.0
X_test <- X_test / 255.0

# Convert Y to categorical
Y_train_cat <- to_categorical(Y_train)
Y_test_cat <- to_categorical(Y_test)
```


```{r}
# Create model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 300, activation = 'tanh', input_shape = c(ncol(X_train))) %>% 
  layer_dense(units = 10, activation = 'softmax')

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_sgd(lr = 0.01),
  metrics = c('accuracy')
)

summary(model)
```

# Part 8

Training neural network with mini-batch gradient descent.

```{r, message=FALSE}
# Train model
history <- model %>% fit(
  X_train, Y_train_cat, 
  epochs = 100 , batch_size = 50, 
  validation_data = list(X_test, Y_test_cat))

plot(history)
```

```{r}
# save learning plot 
png(filename = 'images/part8_learning_curve.png')
plot(history)
dev.off()
```

Displaying 20 digits which were classified correctly

```{r}
# Get predictions on test set
Y_pred <- model %>% predict_classes(X_test)

# Show correct predictions
correct_idx <- Y_pred == Y_test
X_correct <- X_test[correct_idx,]
Y_correct <- Y_test[correct_idx]

par(mfcol = c(2, 10))
par(mar = c(0, 0, 2, 0), pty = 'm')

for (idx in 1:20) {
  x <- X_correct[idx,] %>% rev()
  y <- Y_correct[idx]
  dim(x) <- c(28, 28)
  x <- apply(x, 2, rev)
  
  image(1:28, 1:28, x, col = gray((0:255) / 255),
    xlab = '', ylab = '', xaxt = 'n', yaxt = 'n',
    main = paste('Label: ', y)
  )
}
```

```{r}
# Save image to png
png(filename = 'images/part8_20_correct_digits.png')

par(mfcol = c(2, 10))
par(mar = c(0, 0, 3, 0), pty = 'm', xaxs = 'i', yaxs = 'i')

for (idx in 1:20) {
  x <- X_correct[idx,] %>% rev()
  y <- Y_correct[idx]
  dim(x) <- c(28, 28)
  x <- apply(x, 2, rev)
  
  image(1:28, 1:28, x, col = gray((0:255) / 255),
    xlab = '', ylab = '', xaxt = 'n', yaxt = 'n',
    main = paste('Label: ', y)
  )
}

dev.off()
```

```{r}
# Show incorrect predictions
X_incorrect <- X_test[!correct_idx,]
Y_incorrect <- Y_test[!correct_idx]
Y_pred_incorrect <- Y_pred[!correct_idx]

par(mfcol = c(2, 5))
par(mar = c(0, 0, 2.5, 0), xaxs = 'i', yaxs = 'i')

for (idx in 1:10) {
  x <- X_incorrect[idx,] %>% rev()
  y_true <- Y_incorrect[idx]
  y_pred <- Y_pred_incorrect[idx]
  dim(x) <- c(28, 28)
  x <- apply(x, 2, rev)
  
  image(1:28, 1:28, x, col = gray((0:255) / 255),
    xlab = '', ylab = '', xaxt = 'n', yaxt = 'n',
    main = paste('Label: ', y_true, '\nPred: ', y_pred)
  )
}
```

```{r}
# Save images to png
png(filename = 'images/part8_10_incorrect_digits.png')
par(mfcol = c(2, 5))
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i')

for (idx in 1:10) {
  x <- X_incorrect[idx,] %>% rev()
  y_true <- Y_incorrect[idx]
  y_pred <- Y_pred_incorrect[idx]
  dim(x) <- c(28, 28)
  x <- apply(x, 2, rev)
  
  image(1:28, 1:28, x, col = gray((0:255) / 255),
    xlab = '', ylab = '', xaxt = 'n', yaxt = 'n',
    main = paste('Label: ', y_true, '\nPred: ', y_pred)
  )
}

dev.off()
```


# Part 9

```{r}

# selecting the dense layer 
dense_layer_details <- get_layer(model, index = 1) %>%
  get_weights() 

# selecting the layer with weights for the 300 units
layer_weights <- dense_layer_details[[1]]

# random generator for the 2 out of 300 units
random_index <- sample(1:300, 2, replace=F)

# matrix containing the weights of the first random unit and reshaping into 28x28 matrix
layer1_w <- layer_weights[, random_index[1]] %>%
  matrix(nrow = 28,ncol = 28)
  
# 28x28 matrix with weights of the second random unit
layer2_w <- layer_weights[, random_index[2]] %>%
  matrix(nrow = 28,ncol = 28)


```



```{r}

layer1_viz <- layer1_w %>%    
  # clean up
  as_tibble() %>%
  rowid_to_column(var="x_position") %>%
  gather(key="y_position", value="weight", -1) %>%
  mutate(y_position=as.numeric(gsub("V","",y_position))) %>%
  # graph the heatmap
  ggplot(aes(x_position, y_position, fill= weight)) + 
    geom_tile() +
    theme_minimal() +
    scale_fill_distiller(palette = "PiYG") +
    ggtitle(paste("Heatmap of dense layer unit ", random_index[1])) +
    theme(legend.position="none",
        axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        plot.title = element_text(size=14, face="bold.italic", hjust = 0.5))
  
layer2_viz <- layer2_w %>%    
  # clean up
  as_tibble() %>%
  rowid_to_column(var="x_position") %>%
  gather(key="y_position", value="weight", -1) %>%
  mutate(y_position=as.numeric(gsub("V","",y_position))) %>%
  # graph the heatmap
  ggplot(aes(x_position, y_position, fill= weight)) + 
    geom_tile() +
    theme_minimal() +
    scale_fill_distiller(palette = "PiYG") +
    ggtitle(paste("Heatmap of dense layer unit ", random_index[2])) +
    theme(legend.position="none",
        axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        plot.title = element_text(size=14, face="bold.italic", hjust = 0.5))
  
par(c(1,2))
layer1_viz
layer2_viz

```


















